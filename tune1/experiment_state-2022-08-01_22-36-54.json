{
  "checkpoints": [
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"638a9f48\",\n  \"config\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"local_dir\": \"/home/xwm/DeepSVFilter/code/tune1\",\n  \"evaluated_params\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"experiment_tag\": \"1___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595b6000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944740140000000000008c0347505594473ff000000000000075618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": -1,\n  \"_last_result\": {\n    \"validation_mean\": 0.16666666666666666,\n    \"time_this_iter_s\": 37.95840382575989,\n    \"done\": false,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"trial_id\": \"638a9f48\",\n    \"experiment_id\": \"43007fc922934e7fae290de265a984d1\",\n    \"date\": \"2022-08-01_22-37-41\",\n    \"timestamp\": 1659364661,\n    \"time_total_s\": 37.95840382575989,\n    \"pid\": 58234,\n    \"hostname\": \"localhost.localdomain\",\n    \"node_ip\": \"192.168.249.74\",\n    \"config\": {\n      \"batch_size\": 9,\n      \"beta1\": 0.9,\n      \"beta2\": 0.999,\n      \"lr\": 7.187267009530772e-06,\n      \"weight_decay\": 0.0011614665567890423,\n      \"__trial_index__\": 0\n    },\n    \"time_since_restore\": 37.95840382575989,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"1___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1659364661.137768,\n  \"metric_analysis\": {\n    \"validation_mean\": {\n      \"max\": 0.16666666666666666,\n      \"min\": 0.16666666666666666,\n      \"avg\": 0.16666666666666666,\n      \"last\": 0.16666666666666666,\n      \"last-5-avg\": 0.16666666666666666,\n      \"last-10-avg\": 0.16666666666666666\n    },\n    \"time_this_iter_s\": {\n      \"max\": 37.95840382575989,\n      \"min\": 37.95840382575989,\n      \"avg\": 37.95840382575989,\n      \"last\": 37.95840382575989,\n      \"last-5-avg\": 37.95840382575989,\n      \"last-10-avg\": 37.95840382575989\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 37.95840382575989,\n      \"min\": 37.95840382575989,\n      \"avg\": 37.95840382575989,\n      \"last\": 37.95840382575989,\n      \"last-5-avg\": 37.95840382575989,\n      \"last-10-avg\": 37.95840382575989\n    },\n    \"time_since_restore\": {\n      \"max\": 37.95840382575989,\n      \"min\": 37.95840382575989,\n      \"avg\": 37.95840382575989,\n      \"last\": 37.95840382575989,\n      \"last-5-avg\": 37.95840382575989,\n      \"last-10-avg\": 37.95840382575989\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"validation_mean\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fc5555555555555612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fc5555555555555612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474042faacfa000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474042faacfa000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474042faacfa000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474042faacfa000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474042faacfa000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474042faacfa000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"PENDING\",\n  \"start_time\": 1659364614.7754579,\n  \"logdir\": \"/home/xwm/DeepSVFilter/code/tune1/train_tune_638a9f48_1___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-08-01_22-36-54\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/xwm/DeepSVFilter/code/tune1/train_tune_638a9f48_1___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-08-01_22-36-54/error.txt\",\n  \"error_msg\": \"Traceback (most recent call last):\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trial_runner.py\\\", line 886, in _process_trial\\n    results = self.trial_executor.fetch_result(trial)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\\\", line 675, in fetch_result\\n    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\\\", line 105, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/worker.py\\\", line 1763, in get\\n    raise value.as_instanceof_cause()\\nray.exceptions.RayTaskError(TuneError): \\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=58234, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trainable.py\\\", line 319, in train\\n    result = self.step()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 381, in step\\n    self._report_thread_runner_error(block=True)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 532, in _report_thread_runner_error\\n    (\\\"Trial raised an exception. Traceback:\\\\n{}\\\".format(err_tb_str)\\nray.tune.error.TuneError: Trial raised an exception. Traceback:\\n\\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=58234, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 262, in run\\n    self._entrypoint()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 331, in entrypoint\\n    self._status_reporter.get_checkpoint())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 600, in _trainable_func\\n    output = fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/utils/trainable.py\\\", line 371, in inner\\n    trainable(config, **fn_kwargs)\\n  File \\\"train.py\\\", line 576, in train_tune\\n    trainer.fit(model)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 741, in fit\\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1199, in _run\\n    self._dispatch()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1289, in run_stage\\n    return self._run_train()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1319, in _run_train\\n    self.fit_loop.run()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/fit_loop.py\\\", line 234, in advance\\n    self.epoch_loop.run(data_fetcher)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\\\", line 193, in advance\\n    batch_output = self.batch_loop.run(batch, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\\\", line 88, in advance\\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 219, in advance\\n    self.optimizer_idx,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 266, in _run_optimization\\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 386, in _optimizer_step\\n    using_lbfgs=is_lbfgs,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\\\", line 1652, in optimizer_step\\n    optimizer.step(closure=optimizer_closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py\\\", line 164, in step\\n    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 339, in optimizer_step\\n    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 163, in optimizer_step\\n    optimizer.step(closure=closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/optimizer.py\\\", line 88, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/autograd/grad_mode.py\\\", line 28, in decorate_context\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/adam.py\\\", line 92, in step\\n    loss = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 148, in _wrap_closure\\n    closure_result = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 160, in __call__\\n    self._result = self.closure(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 142, in closure\\n    step_output = self._step_fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 435, in _training_step\\n    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 219, in training_step\\n    return self.training_type_plugin.training_step(*step_kwargs.values())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 213, in training_step\\n    return self.model.training_step(*args, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 444, in training_step\\n    loss, y, y_hat = self.training_validation_step(batch, batch_idx)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 328, in training_validation_step\\n    x2 = self.bert(inputs_embeds=x2)[1]\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 726, in forward\\n    return_dict=return_dict,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 472, in forward\\n    output_hidden_states,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 419, in forward\\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 387, in forward\\n    attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 327, in forward\\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\\nRuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 15.78 GiB total capacity; 3.95 GiB already allocated; 11.50 MiB free; 3.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n\",\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"sync_function_tpl\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059583010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d948c056f72646572944b0075628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9468134b0075628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f948c0a5f6375725f6f72646572944b0075622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"has_new_resources\": false,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"68acdf7c\",\n  \"config\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"local_dir\": \"/home/xwm/DeepSVFilter/code/tune1\",\n  \"evaluated_params\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"experiment_tag\": \"2___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595b6000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944740140000000000008c0347505594473ff000000000000075618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": -1,\n  \"_last_result\": {\n    \"validation_mean\": 0.2777777777777778,\n    \"time_this_iter_s\": 20.913004875183105,\n    \"done\": false,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"trial_id\": \"68acdf7c\",\n    \"experiment_id\": \"14eb961a7102413e8978235d3feefdc9\",\n    \"date\": \"2022-08-01_22-38-11\",\n    \"timestamp\": 1659364691,\n    \"time_total_s\": 20.913004875183105,\n    \"pid\": 58233,\n    \"hostname\": \"localhost.localdomain\",\n    \"node_ip\": \"192.168.249.74\",\n    \"config\": {\n      \"batch_size\": 9,\n      \"beta1\": 0.9,\n      \"beta2\": 0.999,\n      \"lr\": 7.187267009530772e-06,\n      \"weight_decay\": 0.0011614665567890423,\n      \"__trial_index__\": 0\n    },\n    \"time_since_restore\": 20.913004875183105,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"2___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1659364691.995692,\n  \"metric_analysis\": {\n    \"validation_mean\": {\n      \"max\": 0.2777777777777778,\n      \"min\": 0.2777777777777778,\n      \"avg\": 0.2777777777777778,\n      \"last\": 0.2777777777777778,\n      \"last-5-avg\": 0.2777777777777778,\n      \"last-10-avg\": 0.2777777777777778\n    },\n    \"time_this_iter_s\": {\n      \"max\": 20.913004875183105,\n      \"min\": 20.913004875183105,\n      \"avg\": 20.913004875183105,\n      \"last\": 20.913004875183105,\n      \"last-5-avg\": 20.913004875183105,\n      \"last-10-avg\": 20.913004875183105\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 20.913004875183105,\n      \"min\": 20.913004875183105,\n      \"avg\": 20.913004875183105,\n      \"last\": 20.913004875183105,\n      \"last-5-avg\": 20.913004875183105,\n      \"last-10-avg\": 20.913004875183105\n    },\n    \"time_since_restore\": {\n      \"max\": 20.913004875183105,\n      \"min\": 20.913004875183105,\n      \"avg\": 20.913004875183105,\n      \"last\": 20.913004875183105,\n      \"last-5-avg\": 20.913004875183105,\n      \"last-10-avg\": 20.913004875183105\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"validation_mean\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fd1c71c71c71c72612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fd1c71c71c71c72612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034e9bab0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034e9bab0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034e9bab0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034e9bab0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034e9bab0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034e9bab0000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"PENDING\",\n  \"start_time\": 1659364667.3366969,\n  \"logdir\": \"/home/xwm/DeepSVFilter/code/tune1/train_tune_68acdf7c_2___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-08-01_22-37-03\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/xwm/DeepSVFilter/code/tune1/train_tune_68acdf7c_2___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-08-01_22-37-03/error.txt\",\n  \"error_msg\": \"Traceback (most recent call last):\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trial_runner.py\\\", line 886, in _process_trial\\n    results = self.trial_executor.fetch_result(trial)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\\\", line 675, in fetch_result\\n    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\\\", line 105, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/worker.py\\\", line 1763, in get\\n    raise value.as_instanceof_cause()\\nray.exceptions.RayTaskError(TuneError): \\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=58233, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trainable.py\\\", line 319, in train\\n    result = self.step()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 381, in step\\n    self._report_thread_runner_error(block=True)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 532, in _report_thread_runner_error\\n    (\\\"Trial raised an exception. Traceback:\\\\n{}\\\".format(err_tb_str)\\nray.tune.error.TuneError: Trial raised an exception. Traceback:\\n\\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=58233, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 262, in run\\n    self._entrypoint()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 331, in entrypoint\\n    self._status_reporter.get_checkpoint())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 600, in _trainable_func\\n    output = fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/utils/trainable.py\\\", line 371, in inner\\n    trainable(config, **fn_kwargs)\\n  File \\\"train.py\\\", line 576, in train_tune\\n    trainer.fit(model)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 741, in fit\\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1199, in _run\\n    self._dispatch()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1289, in run_stage\\n    return self._run_train()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1319, in _run_train\\n    self.fit_loop.run()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/fit_loop.py\\\", line 234, in advance\\n    self.epoch_loop.run(data_fetcher)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\\\", line 193, in advance\\n    batch_output = self.batch_loop.run(batch, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\\\", line 88, in advance\\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 219, in advance\\n    self.optimizer_idx,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 266, in _run_optimization\\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 386, in _optimizer_step\\n    using_lbfgs=is_lbfgs,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\\\", line 1652, in optimizer_step\\n    optimizer.step(closure=optimizer_closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py\\\", line 164, in step\\n    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 339, in optimizer_step\\n    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 163, in optimizer_step\\n    optimizer.step(closure=closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/optimizer.py\\\", line 88, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/autograd/grad_mode.py\\\", line 28, in decorate_context\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/adam.py\\\", line 92, in step\\n    loss = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 148, in _wrap_closure\\n    closure_result = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 160, in __call__\\n    self._result = self.closure(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 142, in closure\\n    step_output = self._step_fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 435, in _training_step\\n    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 219, in training_step\\n    return self.training_type_plugin.training_step(*step_kwargs.values())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 213, in training_step\\n    return self.model.training_step(*args, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 444, in training_step\\n    loss, y, y_hat = self.training_validation_step(batch, batch_idx)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 328, in training_validation_step\\n    x2 = self.bert(inputs_embeds=x2)[1]\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 726, in forward\\n    return_dict=return_dict,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 472, in forward\\n    output_hidden_states,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 419, in forward\\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 387, in forward\\n    attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 327, in forward\\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\\nRuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 15.78 GiB total capacity; 3.95 GiB already allocated; 11.50 MiB free; 3.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n\",\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"sync_function_tpl\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059583010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d948c056f72646572944b0075628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9468134b0075628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f948c0a5f6375725f6f72646572944b0075622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"has_new_resources\": false,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}"
  ],
  "runner_data": {
    "_insufficient_resources_manager": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "8005958c000000000000008c277261792e74756e652e696e73756666696369656e745f7265736f75726365735f6d616e61676572948c1c496e73756666696369656e745265736f75726365734d616e616765729493942981947d94288c185f6e6f5f72756e6e696e675f747269616c735f73696e6365944741212dc63634bd5d8c0f5f6c6173745f747269616c5f6e756d944b0275622e"
    },
    "_max_pending_trials": 1,
    "_metric": "validation_mean",
    "_total_time": 58.87140870094299,
    "_iteration": 1187,
    "_has_errored": true,
    "_fail_fast": false,
    "_server_port": null,
    "_cached_trial_decisions": {},
    "_queued_trial_decisions": {},
    "_updated_queue": false,
    "_result_wait_time": 1,
    "_should_stop_experiment": false,
    "_local_checkpoint_dir": "/home/xwm/DeepSVFilter/code/tune1",
    "_remote_checkpoint_dir": null,
    "_stopper": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "80059527000000000000008c107261792e74756e652e73746f70706572948c0b4e6f6f7053746f707065729493942981942e"
    },
    "_resumed": false,
    "_start_time": 1659364614.5585744,
    "_last_checkpoint_time": -Infinity,
    "_session_str": "2022-08-01_22-36-54",
    "checkpoint_file": "/home/xwm/DeepSVFilter/code/tune1/experiment_state-2022-08-01_22-36-54.json",
    "_checkpoint_period": "auto",
    "launch_web_server": false
  },
  "stats": {
    "start_time": 1659364614.5585744,
    "timestamp": 1659364685.844887
  }
}