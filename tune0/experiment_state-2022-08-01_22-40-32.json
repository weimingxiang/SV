{
  "checkpoints": [
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"8a77f2f0\",\n  \"config\": {\n    \"batch_size\": 14,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"local_dir\": \"/home/xwm/DeepSVFilter/code/tune0\",\n  \"evaluated_params\": {\n    \"batch_size\": 14,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"experiment_tag\": \"1___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595b6000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944740140000000000008c0347505594473ff000000000000075618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": -1,\n  \"_last_result\": {\n    \"validation_mean\": 0.14285714285714285,\n    \"time_this_iter_s\": 23.299575090408325,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"trial_id\": \"8a77f2f0\",\n    \"experiment_id\": \"4acd4f278c754506bfeadd0346a5a3c5\",\n    \"date\": \"2022-08-01_22-41-30\",\n    \"timestamp\": 1659364890,\n    \"time_total_s\": 23.299575090408325,\n    \"pid\": 66508,\n    \"hostname\": \"localhost.localdomain\",\n    \"node_ip\": \"192.168.249.74\",\n    \"config\": {\n      \"batch_size\": 14,\n      \"beta1\": 0.9,\n      \"beta2\": 0.999,\n      \"lr\": 7.187267009530772e-06,\n      \"weight_decay\": 0.0011614665567890423,\n      \"__trial_index__\": 0\n    },\n    \"time_since_restore\": 23.299575090408325,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"1___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1659364890.8326933,\n  \"metric_analysis\": {\n    \"validation_mean\": {\n      \"max\": 0.35714285714285715,\n      \"min\": 0.14285714285714285,\n      \"avg\": 0.14285714285714285,\n      \"last\": 0.14285714285714285,\n      \"last-5-avg\": 0.25,\n      \"last-10-avg\": 0.25\n    },\n    \"time_this_iter_s\": {\n      \"max\": 23.299575090408325,\n      \"min\": 17.676583528518677,\n      \"avg\": 23.299575090408325,\n      \"last\": 23.299575090408325,\n      \"last-5-avg\": 20.4880793094635,\n      \"last-10-avg\": 20.4880793094635\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 1.0,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1.0,\n      \"last\": 1,\n      \"last-5-avg\": 1.0,\n      \"last-10-avg\": 1.0\n    },\n    \"time_total_s\": {\n      \"max\": 23.299575090408325,\n      \"min\": 17.676583528518677,\n      \"avg\": 23.299575090408325,\n      \"last\": 23.299575090408325,\n      \"last-5-avg\": 20.4880793094635,\n      \"last-10-avg\": 20.4880793094635\n    },\n    \"time_since_restore\": {\n      \"max\": 23.299575090408325,\n      \"min\": 17.676583528518677,\n      \"avg\": 23.299575090408325,\n      \"last\": 23.299575090408325,\n      \"last-5-avg\": 20.4880793094635,\n      \"last-10-avg\": 20.4880793094635\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1.0,\n      \"last\": 1,\n      \"last-5-avg\": 1.0,\n      \"last-10-avg\": 1.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"validation_mean\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fd6db6db6db6db7473fc2492492492492652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fd6db6db6db6db7473fc2492492492492652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031ad34940000004740374cb0f4000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031ad34940000004740374cb0f4000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b01652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b01652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031ad34940000004740374cb0f4000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031ad34940000004740374cb0f4000000652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031ad34940000004740374cb0f4000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031ad34940000004740374cb0f4000000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b01652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b01652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1658760807.6590018,\n  \"logdir\": \"/home/xwm/DeepSVFilter/code/tune0/train_tune_8a77f2f0_1___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-07-25_22-53-27\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/xwm/DeepSVFilter/code/tune0/train_tune_8a77f2f0_1___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-07-25_22-53-27/error.txt\",\n  \"error_msg\": \"Traceback (most recent call last):\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trial_runner.py\\\", line 886, in _process_trial\\n    results = self.trial_executor.fetch_result(trial)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\\\", line 675, in fetch_result\\n    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\\\", line 105, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/worker.py\\\", line 1763, in get\\n    raise value.as_instanceof_cause()\\nray.exceptions.RayTaskError(TuneError): \\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=62669, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trainable.py\\\", line 319, in train\\n    result = self.step()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 381, in step\\n    self._report_thread_runner_error(block=True)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 532, in _report_thread_runner_error\\n    (\\\"Trial raised an exception. Traceback:\\\\n{}\\\".format(err_tb_str)\\nray.tune.error.TuneError: Trial raised an exception. Traceback:\\n\\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=62669, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 262, in run\\n    self._entrypoint()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 331, in entrypoint\\n    self._status_reporter.get_checkpoint())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 600, in _trainable_func\\n    output = fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/utils/trainable.py\\\", line 371, in inner\\n    trainable(config, **fn_kwargs)\\n  File \\\"train.py\\\", line 576, in train_tune\\n    trainer.fit(model)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 741, in fit\\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1199, in _run\\n    self._dispatch()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1289, in run_stage\\n    return self._run_train()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1319, in _run_train\\n    self.fit_loop.run()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/fit_loop.py\\\", line 234, in advance\\n    self.epoch_loop.run(data_fetcher)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\\\", line 193, in advance\\n    batch_output = self.batch_loop.run(batch, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\\\", line 88, in advance\\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 219, in advance\\n    self.optimizer_idx,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 266, in _run_optimization\\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 386, in _optimizer_step\\n    using_lbfgs=is_lbfgs,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\\\", line 1652, in optimizer_step\\n    optimizer.step(closure=optimizer_closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py\\\", line 164, in step\\n    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 339, in optimizer_step\\n    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 163, in optimizer_step\\n    optimizer.step(closure=closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/optimizer.py\\\", line 88, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/autograd/grad_mode.py\\\", line 28, in decorate_context\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/adam.py\\\", line 92, in step\\n    loss = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 148, in _wrap_closure\\n    closure_result = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 160, in __call__\\n    self._result = self.closure(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 142, in closure\\n    step_output = self._step_fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 435, in _training_step\\n    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 219, in training_step\\n    return self.training_type_plugin.training_step(*step_kwargs.values())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 213, in training_step\\n    return self.model.training_step(*args, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 444, in training_step\\n    loss, y, y_hat = self.training_validation_step(batch, batch_idx)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 328, in training_validation_step\\n    x2 = self.bert(inputs_embeds=x2)[1]\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 726, in forward\\n    return_dict=return_dict,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 472, in forward\\n    output_hidden_states,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 419, in forward\\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 387, in forward\\n    attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 320, in forward\\n    mixed_value_layer = self.value(hidden_states)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/linear.py\\\", line 103, in forward\\n    return F.linear(input, self.weight, self.bias)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/functional.py\\\", line 1848, in linear\\n    return torch._C._nn.linear(input, weight, bias)\\nRuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 15.78 GiB total capacity; 9.67 GiB already allocated; 12.50 MiB free; 9.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n\",\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"sync_function_tpl\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059583010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d948c056f72646572944b0075628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9468134b0075628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f948c0a5f6375725f6f72646572944b0075622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"has_new_resources\": false,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"8cafe1a4\",\n  \"config\": {\n    \"batch_size\": 14,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"local_dir\": \"/home/xwm/DeepSVFilter/code/tune0\",\n  \"evaluated_params\": {\n    \"batch_size\": 14,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"experiment_tag\": \"2___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595b6000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944740140000000000008c0347505594473ff000000000000075618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": -1,\n  \"_last_result\": {\n    \"validation_mean\": 0.14285714285714285,\n    \"time_this_iter_s\": 7.689493894577026,\n    \"done\": false,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"trial_id\": \"8cafe1a4\",\n    \"experiment_id\": \"4acd4f278c754506bfeadd0346a5a3c5\",\n    \"date\": \"2022-08-01_22-41-38\",\n    \"timestamp\": 1659364898,\n    \"time_total_s\": 7.689493894577026,\n    \"pid\": 66508,\n    \"hostname\": \"localhost.localdomain\",\n    \"node_ip\": \"192.168.249.74\",\n    \"config\": {\n      \"batch_size\": 14,\n      \"beta1\": 0.9,\n      \"beta2\": 0.999,\n      \"lr\": 7.187267009530772e-06,\n      \"weight_decay\": 0.0011614665567890423,\n      \"__trial_index__\": 0\n    },\n    \"time_since_restore\": 7.689493894577026,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"2___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1659364898.8139467,\n  \"metric_analysis\": {\n    \"validation_mean\": {\n      \"max\": 0.35714285714285715,\n      \"min\": 0.14285714285714285,\n      \"avg\": 0.14285714285714285,\n      \"last\": 0.14285714285714285,\n      \"last-5-avg\": 0.28571428571428575,\n      \"last-10-avg\": 0.28571428571428575\n    },\n    \"time_this_iter_s\": {\n      \"max\": 21.159444093704224,\n      \"min\": 7.689493894577026,\n      \"avg\": 7.689493894577026,\n      \"last\": 7.689493894577026,\n      \"last-5-avg\": 15.609800179799398,\n      \"last-10-avg\": 15.609800179799398\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": 0.0,\n      \"last\": false,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1.0,\n      \"last\": 1,\n      \"last-5-avg\": 1.0,\n      \"last-10-avg\": 1.0\n    },\n    \"time_total_s\": {\n      \"max\": 21.159444093704224,\n      \"min\": 7.689493894577026,\n      \"avg\": 7.689493894577026,\n      \"last\": 7.689493894577026,\n      \"last-5-avg\": 15.609800179799398,\n      \"last-10-avg\": 15.609800179799398\n    },\n    \"time_since_restore\": {\n      \"max\": 21.159444093704224,\n      \"min\": 7.689493894577026,\n      \"avg\": 7.689493894577026,\n      \"last\": 7.689493894577026,\n      \"last-5-avg\": 15.609800179799398,\n      \"last-10-avg\": 15.609800179799398\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1.0,\n      \"last\": 1,\n      \"last-5-avg\": 1.0,\n      \"last-10-avg\": 1.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"validation_mean\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fd6db6db6db6db7473fd6db6db6db6db7473fc2492492492492652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fd6db6db6db6db7473fd6db6db6db6db7473fc2492492492492652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059525000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428898989652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059525000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428898989652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b014b01652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b014b01652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953d000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474031faff9800000047403528d15400000047401ec20ab0000000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b014b01652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b014b01652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"PENDING\",\n  \"start_time\": 1659364771.7558324,\n  \"logdir\": \"/home/xwm/DeepSVFilter/code/tune0/train_tune_8cafe1a4_2___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-07-25_22-53-31\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/xwm/DeepSVFilter/code/tune0/train_tune_8cafe1a4_2___trial_index__=0,batch_size=14,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-07-25_22-53-31/error.txt\",\n  \"error_msg\": \"Traceback (most recent call last):\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trial_runner.py\\\", line 886, in _process_trial\\n    results = self.trial_executor.fetch_result(trial)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\\\", line 675, in fetch_result\\n    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\\\", line 105, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/worker.py\\\", line 1763, in get\\n    raise value.as_instanceof_cause()\\nray.exceptions.RayTaskError(TuneError): \\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=66508, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/trainable.py\\\", line 319, in train\\n    result = self.step()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 381, in step\\n    self._report_thread_runner_error(block=True)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 532, in _report_thread_runner_error\\n    (\\\"Trial raised an exception. Traceback:\\\\n{}\\\".format(err_tb_str)\\nray.tune.error.TuneError: Trial raised an exception. Traceback:\\n\\u001b[36mray::ImplicitFunc.train()\\u001b[39m (pid=66508, ip=192.168.249.74, repr=train_tune)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 262, in run\\n    self._entrypoint()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 331, in entrypoint\\n    self._status_reporter.get_checkpoint())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/function_runner.py\\\", line 600, in _trainable_func\\n    output = fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/ray/tune/utils/trainable.py\\\", line 371, in inner\\n    trainable(config, **fn_kwargs)\\n  File \\\"train.py\\\", line 576, in train_tune\\n    trainer.fit(model)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 741, in fit\\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1199, in _run\\n    self._dispatch()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1289, in run_stage\\n    return self._run_train()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\\\", line 1319, in _run_train\\n    self.fit_loop.run()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/fit_loop.py\\\", line 234, in advance\\n    self.epoch_loop.run(data_fetcher)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\\\", line 193, in advance\\n    batch_output = self.batch_loop.run(batch, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\\\", line 88, in advance\\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\\\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 219, in advance\\n    self.optimizer_idx,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 266, in _run_optimization\\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 386, in _optimizer_step\\n    using_lbfgs=is_lbfgs,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\\\", line 1652, in optimizer_step\\n    optimizer.step(closure=optimizer_closure)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py\\\", line 164, in step\\n    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 339, in optimizer_step\\n    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 163, in optimizer_step\\n    optimizer.step(closure=closure, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/optimizer.py\\\", line 88, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/autograd/grad_mode.py\\\", line 28, in decorate_context\\n    return func(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/optim/adam.py\\\", line 92, in step\\n    loss = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\\\", line 148, in _wrap_closure\\n    closure_result = closure()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 160, in __call__\\n    self._result = self.closure(*args, **kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 142, in closure\\n    step_output = self._step_fn()\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\\\", line 435, in _training_step\\n    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\\\", line 219, in training_step\\n    return self.training_type_plugin.training_step(*step_kwargs.values())\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\\\", line 213, in training_step\\n    return self.model.training_step(*args, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 444, in training_step\\n    loss, y, y_hat = self.training_validation_step(batch, batch_idx)\\n  File \\\"/home/xwm/DeepSVFilter/code/net.py\\\", line 328, in training_validation_step\\n    x2 = self.bert(inputs_embeds=x2)[1]\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 726, in forward\\n    return_dict=return_dict,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 472, in forward\\n    output_hidden_states,\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 419, in forward\\n    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 393, in forward\\n    attention_output[0],\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/modeling_utils.py\\\", line 2928, in apply_chunking_to_forward\\n    return forward_fn(*input_tensors)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/models/albert/modeling_albert.py\\\", line 401, in ff_chunk\\n    ffn_output = self.activation(ffn_output)\\n  File \\\"/home/xwm/anaconda3/envs/SV/lib/python3.6/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/home/xwm/DeepSVFilter/code/transformers/activations.py\\\", line 34, in forward\\n    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\\nRuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 15.78 GiB total capacity; 9.60 GiB already allocated; 64.50 MiB free; 9.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n\",\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"sync_function_tpl\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059583010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d948c056f72646572944b0075628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9468134b0075628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f948c0a5f6375725f6f72646572944b0075622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 3,\n  \"has_new_resources\": false,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"0861318a\",\n  \"config\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"local_dir\": \"/home/xwm/DeepSVFilter/code/tune0\",\n  \"evaluated_params\": {\n    \"batch_size\": 9,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"lr\": 7.187267009530772e-06,\n    \"weight_decay\": 0.0011614665567890423,\n    \"__trial_index__\": 0\n  },\n  \"experiment_tag\": \"3___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595b6000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944740140000000000008c0347505594473ff000000000000075618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": -1,\n  \"_last_result\": {\n    \"validation_mean\": 0.6612347945834289,\n    \"time_this_iter_s\": 2677.1743083000183,\n    \"done\": false,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 12,\n    \"trial_id\": \"0861318a\",\n    \"experiment_id\": \"9a93138ef9ae43f1a8b4272dc7560d73\",\n    \"date\": \"2022-08-02_07-46-10\",\n    \"timestamp\": 1659397570,\n    \"time_total_s\": 32661.898770093918,\n    \"pid\": 66519,\n    \"hostname\": \"localhost.localdomain\",\n    \"node_ip\": \"192.168.249.74\",\n    \"config\": {\n      \"batch_size\": 9,\n      \"beta1\": 0.9,\n      \"beta2\": 0.999,\n      \"lr\": 7.187267009530772e-06,\n      \"weight_decay\": 0.0011614665567890423,\n      \"__trial_index__\": 0\n    },\n    \"time_since_restore\": 32661.898770093918,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 12,\n    \"experiment_tag\": \"3___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1659397570.682892,\n  \"metric_analysis\": {\n    \"validation_mean\": {\n      \"max\": 0.7833371585953638,\n      \"min\": 0.5011093259888303,\n      \"avg\": 0.6142775780140938,\n      \"last\": 0.6612347945834289,\n      \"last-5-avg\": 0.6092265320174433,\n      \"last-10-avg\": 0.6171371738964119\n    },\n    \"time_this_iter_s\": {\n      \"max\": 4136.372336864471,\n      \"min\": 23.465407133102417,\n      \"avg\": 2721.824897507827,\n      \"last\": 2677.1743083000183,\n      \"last-5-avg\": 2675.359508705139,\n      \"last-10-avg\": 2853.4426790714265\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": 0.0,\n      \"last\": false,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"training_iteration\": {\n      \"max\": 12,\n      \"min\": 1,\n      \"avg\": 6.5,\n      \"last\": 12,\n      \"last-5-avg\": 10.0,\n      \"last-10-avg\": 7.5\n    },\n    \"time_total_s\": {\n      \"max\": 32661.898770093918,\n      \"min\": 23.465407133102417,\n      \"avg\": 17494.591874718666,\n      \"last\": 32661.898770093918,\n      \"last-5-avg\": 27313.45506591797,\n      \"last-10-avg\": 20578.416511011124\n    },\n    \"time_since_restore\": {\n      \"max\": 32661.898770093918,\n      \"min\": 23.465407133102417,\n      \"avg\": 17494.591874718666,\n      \"last\": 32661.898770093918,\n      \"last-5-avg\": 27313.45506591797,\n      \"last-10-avg\": 20578.416511011124\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 12,\n      \"min\": 1,\n      \"avg\": 6.5,\n      \"last\": 12,\n      \"last-5-avg\": 10.0,\n      \"last-10-avg\": 7.5\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"validation_mean\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe15c2626f98e72473fe43a0d0b38ef41473fe45d25dbc461d9473fe25dbc461d92e9473fe528d5df36d1c2652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe146d716129b28473fe009166cdaf6d1473fe818e99d2c2033473fe188052ba7ddb0473fe9111916bd13af473fe15c2626f98e72473fe43a0d0b38ef41473fe45d25dbc461d9473fe25dbc461d92e9473fe528d5df36d1c2652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a4f4ebc86800004740a4e20e8ca800004740a4dfd1f15000004740a4e072d25000004740a4ea593ef00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b0285f517800004740a6d20221f000004740a52071095800004740a521ea1bf800004740a50626428000004740a4f4ebc86800004740a4e20e8ca800004740a4dfd1f15000004740a4e072d25000004740a4ea593ef00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898989652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898989652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b084b094b0a4b0b4b0c652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b034b044b054b064b074b084b094b0a4b0b4b0c652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740d573e3f38c00004740d81025c52100004740daac20034b00004740dd482e5d9500004740dfe57985730000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c023ec128e00004740c5d86c9b0a00004740cb2088dd6000004740d03481b22f00004740d2d5467a7f00004740d573e3f38c00004740d81025c52100004740daac20034b00004740dd482e5d9500004740dfe57985730000652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740d573e3f38c00004740d81025c52100004740daac20034b00004740dd482e5d9500004740dfe57985730000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c023ec128e00004740c5d86c9b0a00004740cb2088dd6000004740d03481b22f00004740d2d5467a7f00004740d573e3f38c00004740d81025c52100004740daac20034b00004740dd482e5d9500004740dfe57985730000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b084b094b0a4b0b4b0c652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b034b044b054b064b074b084b094b0a4b0b4b0c652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"RUNNING\",\n  \"start_time\": 1659364904.8983085,\n  \"logdir\": \"/home/xwm/DeepSVFilter/code/tune0/train_tune_0861318a_3___trial_index__=0,batch_size=9,beta1=0.9,beta2=0.999,lr=7.1873e-06,weight_decay=0.0011615_2022-08-01_22-41-44\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"sync_function_tpl\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059583010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d948c056f72646572944b0075628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9468134b0075628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f948c0a5f6375725f6f72646572944b0075622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"has_new_resources\": false,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}"
  ],
  "runner_data": {
    "_insufficient_resources_manager": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "8005958c000000000000008c277261792e74756e652e696e73756666696369656e745f7265736f75726365735f6d616e61676572948c1c496e73756666696369656e745265736f75726365734d616e616765729493942981947d94288c185f6e6f5f72756e6e696e675f747269616c735f73696e6365944741212fa22eca666b8c0f5f6c6173745f747269616c5f6e756d944b0375622e"
    },
    "_max_pending_trials": 1,
    "_metric": "validation_mean",
    "_total_time": 32749.704329252243,
    "_iteration": 47966,
    "_has_errored": true,
    "_fail_fast": false,
    "_server_port": null,
    "_cached_trial_decisions": {},
    "_queued_trial_decisions": {},
    "_updated_queue": false,
    "_result_wait_time": 1,
    "_should_stop_experiment": false,
    "_local_checkpoint_dir": "/home/xwm/DeepSVFilter/code/tune0",
    "_remote_checkpoint_dir": null,
    "_stopper": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "80059527000000000000008c107261792e74756e652e73746f70706572948c0b4e6f6f7053746f707065729493942981942e"
    },
    "_resumed": true,
    "checkpoint_file": "/home/xwm/DeepSVFilter/code/tune0/experiment_state-2022-08-01_22-40-32.json",
    "_session_str": "2022-08-01_22-40-32",
    "_start_time": 1659364832.925848,
    "_last_checkpoint_time": -Infinity,
    "_checkpoint_period": "auto",
    "launch_web_server": false
  },
  "stats": {
    "start_time": 1659364832.925848,
    "timestamp": 1659400131.1900313
  }
}